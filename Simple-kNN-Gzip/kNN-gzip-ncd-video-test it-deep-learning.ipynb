{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kNN+gzip is all you need?! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll load the dataset and split it into train/test groups. I'll start us off with the 500-sample dataset to start, just to keep things running quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 500 \n",
    "\n",
    "with open(f\"sentiment-dataset-{N_SAMPLES}.pickle\", \"rb\") as f:\n",
    "    dataset = pickle.load(f)\n",
    "\n",
    "train_x, train_y, test_x, test_y = dataset # samples are text strings (x) and sentiment -1 or 1 (y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Samples are text strings paired with a label of `-1` for negative sentiment or `1` for positive sentiment. For example, we can check our first sample's text and sentiment label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous reviewer Claudio Carvalho gave a much better recap of the film's plot details than I could. What I recall mostly is that it was just so beautiful, in every sense - emotionally, visually, editorially - just gorgeous.<br /><br />If you like movies that are wonderful to look at, and also have emotional content to which that beauty is relevant, I think you will be glad to have seen this extraordinary and unusual work of art.<br /><br />On a scale of 1 to 10, I'd give it about an 8.75. The only reason I shy away from 9 is that it is a mood piece. If you are in the mood for a really artistic, very romantic film, then it's a 10. I definitely think it's a must-see, but none of us can be in that mood all the time, so, overall, 8.75.\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(train_x[0])\n",
    "print(train_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning models don't work with text though, we need to represent text in some way numerically instead and somehow make it a \"feature\" that a model could fit to. The proposal is that we compress and use NCD, or normalized compression distances. \n",
    "\n",
    "What's that? We want to first compress the text, something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b\"\\x1f\\x8b\\x08\\x00\\x13\\xd1\\xbad\\x02\\xffUR\\xbb\\x8e\\xdb0\\x10\\xfc\\x95\\xe9\\xdc\\xe8\\x9cs\\x11$\\x01\\x824W\\xb9\\xba\\x14\\x01R\\xaf\\xa4\\x95\\xb51\\xc55HJ\\x8a\\xff>C\\xda\\x08\\x92\\x86\\x12\\xc9\\xdd\\xd9y\\xf0{\\xd2\\xcd|\\xcd\\xa8_\\xdd5\\xe1-\\xc8:\\x9a\\xe3M\\xd2&av\\\\dS\\x08\\x96u\\x98\\xd1k)\\xacI:\\xc8\\r>\\xa1\\xcc\\x8a\\xc9\\xc2r\\xc8\\xb8\\x05/\\x18\\xb5\\x88\\x85\\xccs\\x898c\\xf05\\x8cG\\xfc\\x9c\\xa5pW\\xbbB\\xc0\\xe2\\xb9\\x84;\\xacU\\x15X\\xc1.\\x19\\xbf\\xd6\\\\\\x90\\x9d\\x13d-6\\xad\\xa1\\x83E\\xe8\\xa6\\xe9\\x8e\\xac1+^\\xa0\\x8b\\x17\\xf3H\\x90{\\x87\\xcd\\xf2\\xfa\\xf8\\xd3\\xd1\\x8a'\\xab\\x1b\\x165\\xa0\\x8b\\xa7\\x8bR\\xd6\\xf1k\\x9f\\xf0\\xe1\\xdbc=O\\xb8\\xfb\\x8a`W%\\t\\xca}2\\x90\\xa4\\xd8=\\x8e\\x9a8\\x16\\xc5\\x11\\xdc\\xaf\\x90\\xd2A\\xe2\\x08\\td5W\\x0f\\xfe\\x8e\\xa7\\xaeX4\\x96Z\\xbb\\xcfF_\\x1aN\\xa3\\xde\\x84%\\r\\xbaI$\\xc2\\x99W\\x16\\xafm\\xf0nT\\xdf+.A\\xc6\\xda\\xda@\\xb3j\\xac5\\x19\\xfa\\xbb$\\xf14Z\\x14j\\xae\\xa3\\xd7\\xb8V\\x8d$\\x97\\xae\\xd5mI\\xe5?A\\xef\\x91\\xb9d\\x9a\\xaa\\xf5\\xf6T1O\\xaf\\x9cy\\x18q1b\\xd3Z\\xe9}\\xe5\\x1a\\xf1\\xf9\\xf8\\xe9\\xe3\\x11?\\x98\\x97G\\xfa\\x94T\\xb2\\xd7\\x88\\xf2\\xccY\\xbb\\xdc1%_\\xf0\\xe5\\xdfX\\xf8\\xcb\\xd8\\xddG\\xdcL\\x07=\\xe2i`\\xf5\\xcbb\\xcb\\xbe\\xddN\\x9eXH\\xc4\\x1a\\x009Z.60\\xa0\\x1a\\x1dA\\xe9\\x83\\r\\xed\\x95t\\xb5'\\x12\\xfaP\\x91O\\xafD\\xe4\\x8b\\x99,ZQ\\xb6>\\x9cz\\xde.\\x8c\\xf1\\x85\\xe6t\\xe8) zl\\x1a\\xf9P\\x07\\x8a\\xe9\\x9f\\x04\\xc8\\xb31\\xa8\\xcf\\xaa\\xd2)\\xb6\\xb0!{\\x07\\xe7t\\x9ev\\x0f\\xdd\\x7f\\x00hIJ\\xac\\xe6\\x02\\x00\\x00\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_compressed = gzip.compress(train_x[0].encode())\n",
    "x_compressed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesn't look like any number to me! Instead, we'll convert it to a number by taking the length of the compressed text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "451"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_compressed = len(gzip.compress(train_x[0].encode()))\n",
    "x_compressed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't just use the size of the compressed text, because that's not normalized and easily comparable. We want to normalize it so that we can compare the distances between two compressed texts and essentially the entire dataset.\n",
    "\n",
    "So we'd grab the length of another compressed sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "639"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2_compressed = len(gzip.compress(train_x[1].encode())) \n",
    "x2_compressed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll do this to the combined strings, so x plus x2 as `xx2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx2 = len(gzip.compress((\" \".join([train_x[0],train_x[1]])).encode()))\n",
    "xx2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay. So, we have a number, but it's not normalized, so we'll set the value to be:\n",
    "\n",
    "The combined sample compression length minus the lesser of the two sample compressed lengths, divided by the largest of the two sample compressed lengths. Like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8967136150234741"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ncd = (xx2 - min(x_compressed, x2_compressed)) / max(x_compressed, x2_compressed)\n",
    "ncd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wonderful, looks like a number to me! Now we want to do this with every sample in the dataset, and every sample needs to be compared to every other sample, so our NCD function will look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NCD with compressed lengths\n",
    "def ncd(x, x2):\n",
    "  x_compressed = len(gzip.compress(x.encode()))\n",
    "  x2_compressed = len(gzip.compress(x2.encode()))  \n",
    "  xx2 = len(gzip.compress((\" \".join([x,x2])).encode()))\n",
    "  return (xx2 - min(x_compressed, x2_compressed)) / max(x_compressed, x2_compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll compute the NCD for all training samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ncd = [[ncd(train_x[i], train_x[j]) for j in range(len(train_x))] for i in range(len(train_x))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, so let's see an example of a sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "401\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.026607538802660754,\n",
       " 0.8967136150234741,\n",
       " 0.8337028824833703,\n",
       " 0.844789356984479,\n",
       " 0.83991683991684,\n",
       " 0.8226164079822617,\n",
       " 0.8969072164948454,\n",
       " 0.8021505376344086,\n",
       " 0.8631790744466801,\n",
       " 0.8625277161862528,\n",
       " 0.8048780487804879,\n",
       " 0.8824833702882483,\n",
       " 0.862992125984252,\n",
       " 0.8520408163265306,\n",
       " 0.8512544802867383,\n",
       " 0.835920177383592,\n",
       " 0.8603104212860311,\n",
       " 0.811529933481153,\n",
       " 0.8580931263858093,\n",
       " 0.8755980861244019,\n",
       " 0.8181818181818182,\n",
       " 0.8314855875831486,\n",
       " 0.9324866310160428,\n",
       " 0.8489361702127659,\n",
       " 0.9553372041089773,\n",
       " 0.8717105263157895,\n",
       " 0.8492239467849224,\n",
       " 0.9112627986348123,\n",
       " 0.8381374722838137,\n",
       " 0.8470066518847007,\n",
       " 0.8350515463917526,\n",
       " 0.8425720620842572,\n",
       " 0.8871866295264624,\n",
       " 0.9157581764122894,\n",
       " 0.835920177383592,\n",
       " 0.8898963730569949,\n",
       " 0.9588457899716177,\n",
       " 0.8514412416851441,\n",
       " 0.8217391304347826,\n",
       " 0.85,\n",
       " 0.8582089552238806,\n",
       " 0.8536585365853658,\n",
       " 0.835920177383592,\n",
       " 0.8615384615384616,\n",
       " 0.8292682926829268,\n",
       " 0.9052734375,\n",
       " 0.8625277161862528,\n",
       " 0.8812392426850258,\n",
       " 0.9430455635491607,\n",
       " 0.8381374722838137,\n",
       " 0.9491358024691358,\n",
       " 0.844789356984479,\n",
       " 0.925849639546859,\n",
       " 0.8514412416851441,\n",
       " 0.8314855875831486,\n",
       " 0.8098290598290598,\n",
       " 0.9269406392694064,\n",
       " 0.8326086956521739,\n",
       " 0.9592743428359867,\n",
       " 0.8480492813141683,\n",
       " 0.9333826794966691,\n",
       " 0.9278969957081545,\n",
       " 0.8691796008869179,\n",
       " 0.8669623059866962,\n",
       " 0.8425720620842572,\n",
       " 0.8741258741258742,\n",
       " 0.8580562659846548,\n",
       " 0.9195046439628483,\n",
       " 0.8536585365853658,\n",
       " 0.8203991130820399,\n",
       " 0.9201277955271565,\n",
       " 0.8373015873015873,\n",
       " 0.8767361111111112,\n",
       " 0.8823529411764706,\n",
       " 0.8270509977827051,\n",
       " 0.8663446054750402,\n",
       " 0.8839009287925697,\n",
       " 0.8392857142857143,\n",
       " 0.8295019157088123,\n",
       " 0.8463035019455253,\n",
       " 0.8492239467849224,\n",
       " 0.8609865470852018,\n",
       " 0.9063636363636364,\n",
       " 0.8189845474613686,\n",
       " 0.8226164079822617,\n",
       " 0.8773747841105354,\n",
       " 0.9227974568574023,\n",
       " 0.9525888958203369,\n",
       " 0.82560706401766,\n",
       " 0.9188224799286352,\n",
       " 0.835920177383592,\n",
       " 0.865625,\n",
       " 0.8691796008869179,\n",
       " 0.8382899628252788,\n",
       " 0.9049826187717266,\n",
       " 0.9534883720930233,\n",
       " 0.8824306472919419,\n",
       " 0.8661870503597122,\n",
       " 0.8657487091222031,\n",
       " 0.835920177383592,\n",
       " 0.9261477045908184,\n",
       " 0.9015837104072398,\n",
       " 0.8558758314855875,\n",
       " 0.8613518197573656,\n",
       " 0.8548707753479126,\n",
       " 0.9030303030303031,\n",
       " 0.8558758314855875,\n",
       " 0.873758865248227,\n",
       " 0.9336206896551724,\n",
       " 0.8586065573770492,\n",
       " 0.8248337028824834,\n",
       " 0.8514412416851441,\n",
       " 0.8330134357005758,\n",
       " 0.8230088495575221,\n",
       " 0.8203991130820399,\n",
       " 0.8473581213307241,\n",
       " 0.8248337028824834,\n",
       " 0.9344709897610921,\n",
       " 0.8863976083707026,\n",
       " 0.9656322989656323,\n",
       " 0.8290598290598291,\n",
       " 0.8764044943820225,\n",
       " 0.8870056497175142,\n",
       " 0.9311497326203209,\n",
       " 0.9442622950819672,\n",
       " 0.87518573551263,\n",
       " 0.950965824665676,\n",
       " 0.8440860215053764,\n",
       " 0.8935309973045822,\n",
       " 0.900871459694989,\n",
       " 0.8625277161862528,\n",
       " 0.9569183983781044,\n",
       " 0.8625277161862528,\n",
       " 0.887719298245614,\n",
       " 0.907537688442211,\n",
       " 0.9313087490961678,\n",
       " 0.9053892215568863,\n",
       " 0.8852459016393442,\n",
       " 0.9087837837837838,\n",
       " 0.8558758314855875,\n",
       " 0.8608695652173913,\n",
       " 0.8774509803921569,\n",
       " 0.9186923721709975,\n",
       " 0.8929016189290162,\n",
       " 0.8579654510556622,\n",
       " 0.8398268398268398,\n",
       " 0.8691796008869179,\n",
       " 0.8381374722838137,\n",
       " 0.8654781199351702,\n",
       " 0.8456659619450317,\n",
       " 0.9233226837060703,\n",
       " 0.838495575221239,\n",
       " 0.8679577464788732,\n",
       " 0.9036402569593148,\n",
       " 0.9085872576177285,\n",
       " 0.8492239467849224,\n",
       " 0.8514412416851441,\n",
       " 0.8425720620842572,\n",
       " 0.8511796733212341,\n",
       " 0.9179170344218888,\n",
       " 0.8470066518847007,\n",
       " 0.8516949152542372,\n",
       " 0.835920177383592,\n",
       " 0.8226164079822617,\n",
       " 0.8859259259259259,\n",
       " 0.8580931263858093,\n",
       " 0.902229845626072,\n",
       " 0.8514412416851441,\n",
       " 0.8923076923076924,\n",
       " 0.8292682926829268,\n",
       " 0.9358288770053476,\n",
       " 0.8337028824833703,\n",
       " 0.835920177383592,\n",
       " 0.8603104212860311,\n",
       " 0.8536585365853658,\n",
       " 0.8647450110864745,\n",
       " 0.9079401611047181,\n",
       " 0.8159645232815964,\n",
       " 0.8580931263858093,\n",
       " 0.8314855875831486,\n",
       " 0.8822674418604651,\n",
       " 0.9510143493320139,\n",
       " 0.9019607843137255,\n",
       " 0.8470066518847007,\n",
       " 0.9542168674698795,\n",
       " 0.8858800773694391,\n",
       " 0.8492239467849224,\n",
       " 0.8603104212860311,\n",
       " 0.8625277161862528,\n",
       " 0.9122055674518201,\n",
       " 0.83991683991684,\n",
       " 0.8732612055641422,\n",
       " 0.851063829787234,\n",
       " 0.8722689075630252,\n",
       " 0.8833592534992224,\n",
       " 0.8770491803278688,\n",
       " 0.9169295478443743,\n",
       " 0.8558758314855875,\n",
       " 0.8416666666666667,\n",
       " 0.8558758314855875,\n",
       " 0.9119420989143546,\n",
       " 0.8774002954209749,\n",
       " 0.8521739130434782,\n",
       " 0.8691796008869179,\n",
       " 0.9242685025817556,\n",
       " 0.8580931263858093,\n",
       " 0.835920177383592,\n",
       " 0.9309859154929577,\n",
       " 0.8957871396895787,\n",
       " 0.8203991130820399,\n",
       " 0.8226164079822617,\n",
       " 0.9504132231404959,\n",
       " 0.9235033259423503,\n",
       " 0.859304084720121,\n",
       " 0.8580931263858093,\n",
       " 0.8688524590163934,\n",
       " 0.9293193717277487,\n",
       " 0.9276054097056484,\n",
       " 0.8709016393442623,\n",
       " 0.90625,\n",
       " 0.8625277161862528,\n",
       " 0.8625277161862528,\n",
       " 0.8594594594594595,\n",
       " 0.8516377649325626,\n",
       " 0.8292682926829268,\n",
       " 0.9249812453113279,\n",
       " 0.8940149625935162,\n",
       " 0.9078624078624079,\n",
       " 0.8492239467849224,\n",
       " 0.8381374722838137,\n",
       " 0.8903061224489796,\n",
       " 0.872,\n",
       " 0.8314855875831486,\n",
       " 0.8536585365853658,\n",
       " 0.897982062780269,\n",
       " 0.8492239467849224,\n",
       " 0.93503800967519,\n",
       " 0.9167717528373266,\n",
       " 0.8603104212860311,\n",
       " 0.9060324825986079,\n",
       " 0.8270509977827051,\n",
       " 0.811529933481153,\n",
       " 0.8174097664543525,\n",
       " 0.9486500794070937,\n",
       " 0.9330007127583749,\n",
       " 0.8497576736672051,\n",
       " 0.8603104212860311,\n",
       " 0.8292682926829268,\n",
       " 0.9316298342541437,\n",
       " 0.8208955223880597,\n",
       " 0.9162833486660533,\n",
       " 0.8093126385809313,\n",
       " 0.8137472283813747,\n",
       " 0.9649202733485194,\n",
       " 0.8723994452149791,\n",
       " 0.8952496954933008,\n",
       " 0.8492239467849224,\n",
       " 0.8425720620842572,\n",
       " 0.9094827586206896,\n",
       " 0.8226164079822617,\n",
       " 0.8403547671840355,\n",
       " 0.8470066518847007,\n",
       " 0.8625277161862528,\n",
       " 0.8558758314855875,\n",
       " 0.8070953436807096,\n",
       " 0.8945409429280397,\n",
       " 0.8669623059866962,\n",
       " 0.844789356984479,\n",
       " 0.8713968957871396,\n",
       " 0.8779472954230236,\n",
       " 0.8701030927835052,\n",
       " 0.888,\n",
       " 0.8270509977827051,\n",
       " 0.8641571194762684,\n",
       " 0.8780487804878049,\n",
       " 0.8381374722838137,\n",
       " 0.8137472283813747,\n",
       " 0.9554384283660757,\n",
       " 0.8462897526501767,\n",
       " 0.9386026817219478,\n",
       " 0.9276504297994269,\n",
       " 0.8925869894099848,\n",
       " 0.8881987577639752,\n",
       " 0.8647450110864745,\n",
       " 0.8471953578336557,\n",
       " 0.8181818181818182,\n",
       " 0.9077380952380952,\n",
       " 0.8479087452471483,\n",
       " 0.8779411764705882,\n",
       " 0.9480375898286346,\n",
       " 0.8286334056399133,\n",
       " 0.8761904761904762,\n",
       " 0.8822553897180763,\n",
       " 0.8638743455497382,\n",
       " 0.8381374722838137,\n",
       " 0.808695652173913,\n",
       " 0.8814531548757171,\n",
       " 0.9527220630372493,\n",
       " 0.8461538461538461,\n",
       " 0.9340909090909091,\n",
       " 0.9097387173396675,\n",
       " 0.884080370942813,\n",
       " 0.8314855875831486,\n",
       " 0.8558758314855875,\n",
       " 0.8536585365853658,\n",
       " 0.9367854741089442,\n",
       " 0.8514412416851441,\n",
       " 0.9071644803229062,\n",
       " 0.9274809160305344,\n",
       " 0.8181818181818182,\n",
       " 0.8713968957871396,\n",
       " 0.9089301503094607,\n",
       " 0.8403547671840355,\n",
       " 0.8568738229755178,\n",
       " 0.8669623059866962,\n",
       " 0.8913525498891353,\n",
       " 0.8685446009389671,\n",
       " 0.8457350272232305,\n",
       " 0.8913525498891353,\n",
       " 0.8386454183266933,\n",
       " 0.9181395348837209,\n",
       " 0.8603104212860311,\n",
       " 0.8137472283813747,\n",
       " 0.9042253521126761,\n",
       " 0.8871428571428571,\n",
       " 0.835920177383592,\n",
       " 0.8492239467849224,\n",
       " 0.8385093167701864,\n",
       " 0.9526909722222222,\n",
       " 0.8736141906873615,\n",
       " 0.8492239467849224,\n",
       " 0.8470066518847007,\n",
       " 0.887987012987013,\n",
       " 0.932788374205268,\n",
       " 0.8736141906873615,\n",
       " 0.8713968957871396,\n",
       " 0.8603104212860311,\n",
       " 0.8713968957871396,\n",
       " 0.9215143120960295,\n",
       " 0.9401709401709402,\n",
       " 0.8532494758909853,\n",
       " 0.9051254089422028,\n",
       " 0.8492239467849224,\n",
       " 0.8470066518847007,\n",
       " 0.8873015873015873,\n",
       " 0.8203991130820399,\n",
       " 0.8226164079822617,\n",
       " 0.8924870466321243,\n",
       " 0.9461388708630759,\n",
       " 0.8793650793650793,\n",
       " 0.8270509977827051,\n",
       " 0.8701517706576728,\n",
       " 0.8433476394849786,\n",
       " 0.8625277161862528,\n",
       " 0.8625277161862528,\n",
       " 0.9138858988159311,\n",
       " 0.9227941176470589,\n",
       " 0.8580931263858093,\n",
       " 0.8514412416851441,\n",
       " 0.9047085201793722,\n",
       " 0.8048780487804879,\n",
       " 0.9245463228271251,\n",
       " 0.9199048374306106,\n",
       " 0.8470066518847007,\n",
       " 0.8402489626556017,\n",
       " 0.835920177383592,\n",
       " 0.861904761904762,\n",
       " 0.8857938718662952,\n",
       " 0.8403547671840355,\n",
       " 0.8556034482758621,\n",
       " 0.8558758314855875,\n",
       " 0.864406779661017,\n",
       " 0.8691796008869179,\n",
       " 0.8967828418230563,\n",
       " 0.8203991130820399,\n",
       " 0.8833333333333333,\n",
       " 0.8866396761133604,\n",
       " 0.8666666666666667,\n",
       " 0.8794835007173601,\n",
       " 0.844789356984479,\n",
       " 0.9269893355209188,\n",
       " 0.8892455858747994,\n",
       " 0.8492239467849224,\n",
       " 0.843010752688172,\n",
       " 0.8270509977827051,\n",
       " 0.9056795131845842,\n",
       " 0.8425720620842572,\n",
       " 0.9200376293508937,\n",
       " 0.8910256410256411,\n",
       " 0.8425720620842572,\n",
       " 0.8827470686767169,\n",
       " 0.881578947368421,\n",
       " 0.8349900596421471,\n",
       " 0.8619631901840491,\n",
       " 0.8558758314855875,\n",
       " 0.8937728937728938,\n",
       " 0.8956714761376249,\n",
       " 0.8492239467849224,\n",
       " 0.8337028824833703,\n",
       " 0.8708133971291866,\n",
       " 0.8470066518847007]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_ncd[0]))\n",
    "train_ncd[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 501 total samples, we allocated 401 to training and the other 100 to testing in this case. You can see here for the first sample, all the distances are 0.8-0.9s, but we even compare it to itself, so it's extremely close. The only reason for any distance at all is how we're calculating NCD.\n",
    "\n",
    "So each sample is a vector of 401 normalized compression distances. We'll do the same thing with the test data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ncd = [[ncd(test_x[i], train_x[j]) for j in range(len(train_x))] for i in range(len(test_x))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that here we are calculating the normalized compression distances between the testing samples and training samples. Later, when you would go to actually deploy this into the wild so to speak, you would do the same. Every sample would be compared to every sample to build the vector of normalized compression distances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, with that, we're ready to train a classifier apparently!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is typical with kNN, feel free to tinker with the number of neighbors. The original paper was using 2. I found 2 to be far too variable, but there are just so many factors to consider here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=7)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors=7)\n",
    "neigh.fit(train_ncd, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see and note by speeds, the slowest part of this algorithm is computing the NCDs. The actual kNN classification is extremely fast, especially on just 500 samples. We'll address the NCD calculation speed shortly... but how did we do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7029702970297029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/h/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", neigh.score(test_ncd, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so we're not beating BeRT, but we're not doing too bad either. We're also not using any fancy techniques, we're just using kNN with gzip compression and NCDs and 500 samples. And we did it in 25 lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7029702970297029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/h/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pickle\n",
    "\n",
    "\n",
    "N_SAMPLES = 500 \n",
    "\n",
    "with open(f\"sentiment-dataset-{N_SAMPLES}.pickle\", \"rb\") as f:\n",
    "    dataset = pickle.load(f)\n",
    "\n",
    "train_x, train_y, test_x, test_y = dataset # samples are text strings (x) and sentiment -1 or 1 (y)\n",
    "\n",
    "def ncd(x, x2): # NCD with compressed lengths\n",
    "    x_compressed = len(gzip.compress(x.encode()))\n",
    "    x2_compressed = len(gzip.compress(x2.encode()))  \n",
    "    xx2 = len(gzip.compress((\" \".join([x,x2])).encode()))\n",
    "    return (xx2 - min(x_compressed, x2_compressed)) / max(x_compressed, x2_compressed)\n",
    "\n",
    "train_ncd = [[ncd(train_x[i], train_x[j]) for j in range(len(train_x))] for i in range(len(train_x))]\n",
    "test_ncd = [[ncd(test_x[i], train_x[j]) for j in range(len(train_x))] for i in range(len(test_x))]\n",
    "\n",
    "# KNN classification\n",
    "neigh = KNeighborsClassifier(n_neighbors=7) \n",
    "neigh.fit(train_ncd, train_y)\n",
    "print(\"Accuracy:\", neigh.score(test_ncd, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import gzip\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import multiprocessing\n",
    "\n",
    "# Number of processes to use\n",
    "NUM_PROCESSES = 56 # modify based on your avail CPU cores\n",
    "N_SAMPLES = 500  # 500, 1500, 5000, OR 10000 Pre-created\n",
    "\n",
    "with open(f\"sentiment-dataset-{N_SAMPLES}.pickle\", \"rb\") as f:\n",
    "    dataset = pickle.load(f)\n",
    "\n",
    "train_x, train_y, test_x, test_y = dataset\n",
    "\n",
    "# NCD with compressed lengths\n",
    "def ncd(x, x2):\n",
    "    x_compressed = len(gzip.compress(x.encode()))\n",
    "    x2_compressed = len(gzip.compress(x2.encode()))  \n",
    "    xx2 = len(gzip.compress((\" \".join([x,x2])).encode()))\n",
    "    return (xx2 - min(x_compressed, x2_compressed)) / max(x_compressed, x2_compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we know we want to parallelize this process, and how we might do that can vary. We are going to calculate NCD for every single sample, and for every single sample, against every sample. \n",
    "\n",
    "No calculation there depends on any other calculation, but it's absolutely imperative that the order of samples and the NCD \"vector\" for each sample is maintained. \n",
    "\n",
    "To do this, we'll use multiprocessing's `Pool` to calculate those NCD vector values, but if we just did this and appended the values to a list, even if we initiated the pools in order, we'd have no guarantee that the order of the samples would be maintained as some processes may finish before others.\n",
    "\n",
    "To handle for that, we'll first initialize the NCD matrices with 0s and then update them according to indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize NCD matrices  \n",
    "train_ncd = [[0] * len(train_x) for _ in range(len(train_x))]\n",
    "test_ncd = [[0] * len(train_x) for _ in range(len(test_x))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will specify a helper function that calculates the NCDs for a given sample, and then we'll use `Pool` to map that function to all samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to compute NCD row \n",
    "def calculate_ncd_row(data_row): # data row is a tuple of (index, text)\n",
    "    i = data_row[0] # index of the data and data_row[1] is the text data\n",
    "    # calcs the row of NCD values for the given data sample\n",
    "    row = [ncd(data_row[1], train_x[j]) for j in range(len(train_x))]\n",
    "    return i, row # return index and row's ncd values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the pool and calculate the values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with multiprocessing.Pool(NUM_PROCESSES) as pool:\n",
    "    # Compute train NCD\n",
    "    train_data = enumerate(train_x)\n",
    "    train_results = pool.map(calculate_ncd_row, train_data)\n",
    "    \n",
    "    # Compute test NCD\n",
    "    test_data = enumerate(test_x)  \n",
    "    test_results = pool.map(calculate_ncd_row, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we know, for each index, the vector of NCD values, so then we can populate them into those zeroed matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert rows into NCD matrices    \n",
    "for i, row in train_results:\n",
    "    train_ncd[i] = row\n",
    "    \n",
    "for i, row in test_results:\n",
    "    test_ncd[i] = row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can train and test the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7029702970297029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/h/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "# KNN classification\n",
    "neigh = KNeighborsClassifier(n_neighbors=7)\n",
    "neigh.fit(train_ncd, train_y)\n",
    "\n",
    "accuracy = neigh.score(test_ncd, test_y)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can confirm that's the same accuracy as before, so it looks like our logic works. Now let's run this all together and use the `10000` sample dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7571214392803598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/h/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "import gzip\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import multiprocessing\n",
    "\n",
    "# Number of processes to use\n",
    "NUM_PROCESSES = 56\n",
    "N_SAMPLES = 10000  # 500, 1500, 5000, OR 10000 Pre-created\n",
    "\n",
    "with open(f\"sentiment-dataset-{N_SAMPLES}.pickle\", \"rb\") as f:\n",
    "    dataset = pickle.load(f)\n",
    "\n",
    "train_x, train_y, test_x, test_y = dataset\n",
    "\n",
    "# NCD with compressed lengths\n",
    "def ncd(x, x2):\n",
    "  x_compressed = len(gzip.compress(x.encode()))\n",
    "  x2_compressed = len(gzip.compress(x2.encode()))  \n",
    "  xx2 = len(gzip.compress((\" \".join([x,x2])).encode()))\n",
    "  return (xx2 - min(x_compressed, x2_compressed)) / max(x_compressed, x2_compressed)\n",
    "\n",
    "# Initialize NCD matrices  \n",
    "train_ncd = [[0] * len(train_x) for _ in range(len(train_x))]\n",
    "test_ncd = [[0] * len(train_x) for _ in range(len(test_x))]\n",
    "\n",
    "# Helper function to compute NCD row \n",
    "def calculate_ncd_row(data_row):\n",
    "    i = data_row[0]\n",
    "    row = [ncd(data_row[1], train_x[j]) for j in range(len(train_x))] \n",
    "    return i, row\n",
    "\n",
    "with multiprocessing.Pool(NUM_PROCESSES) as pool:\n",
    "    # Compute train NCD\n",
    "    train_data = enumerate(train_x)\n",
    "    train_results = pool.map(calculate_ncd_row, train_data)\n",
    "    \n",
    "    # Compute test NCD\n",
    "    test_data = enumerate(test_x)  \n",
    "    test_results = pool.map(calculate_ncd_row, test_data)\n",
    "    \n",
    "# Insert rows into NCD matrices    \n",
    "for i, row in train_results:\n",
    "    train_ncd[i] = row\n",
    "    \n",
    "for i, row in test_results:\n",
    "    test_ncd[i] = row\n",
    "\n",
    "# KNN classification\n",
    "neigh = KNeighborsClassifier(n_neighbors=7)\n",
    "neigh.fit(train_ncd, train_y)\n",
    "\n",
    "accuracy = neigh.score(test_ncd, test_y)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`75.7%` accuracy is not bad at all for such a simple method! That's very cool. It's still less accuracy than the original paper's authors, so it'd be worth looking into their methods to see what they did differently and maybe even help to inform you of some ideas about how to even further improve the method in general.\n",
    "\n",
    "Ok, so of course the next question is, okay, how might we use such a method in practice? It's fine that we've trained a model, but how do we make a new prediction?\n",
    "\n",
    "Essentially, it's the same as any other sample. You will take your your input string, compress it, and compare the distances to every other sample in the training set. Then we'll pass it through kNN and determine if it has more postive or negative sentiment neighbors and then we'll classify it as such. I have 2 example samples: \n",
    "\n",
    "\n",
    "Positive:\n",
    "```\n",
    "\"I was amazed by the breathtaking beauty of the sunset at the beach. The vibrant hues of orange, pink, and gold painted the sky, creating a mesmerizing scene. It was a moment of pure serenity and joy, where I felt completely at peace and in awe of nature's wonders.\" \n",
    "```\n",
    "\n",
    "Negative:\n",
    "```\n",
    "\"I was deeply disappointed with the customer service I received from the airline. The representatives were dismissive, unprofessional, and completely indifferent to my concerns. It was a frustrating and disheartening experience that left me with a sour taste in my mouth.\"\n",
    "```\n",
    "\n",
    "So, for a sample, you might have it defined like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x = \"I was amazed by the breathtaking beauty of the sunset at the beach. The vibrant hues of orange, pink, and gold painted the sky, creating a mesmerizing scene. It was a moment of pure serenity and joy, where I felt completely at peace and in awe of nature's wonders.\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we compress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x_compressed = len(gzip.compress(new_x.encode()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we calculate the NCD vector for this sample, which is the calculation of NCDs between this sample and every other sample in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ncd = [ncd(new_x, train_x[i]) for i in range(len(train_x))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could multiprocess there as well, but it's fast enough for me for a single sample, and we already have the code ready and waiting for batch inference, should we desire to do more simultaneously. \n",
    "\n",
    "Then we can pass it through the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/h/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "pred = neigh.predict([new_ncd])\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in this case, the prediction is a positive sentiment (1 for positive, -1 for negative). Let's try the negative sentiment string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/h/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "new_x = \"I was deeply disappointed with the customer service I received from the airline. The representatives were dismissive, unprofessional, and completely indifferent to my concerns. It was a frustrating and disheartening experience that left me with a sour taste in my mouth.\"\n",
    "# Compress\n",
    "new_x_compressed = len(gzip.compress(new_x.encode()))\n",
    "\n",
    "# Calculate NCD\n",
    "new_ncd = [ncd(new_x, train_x[i]) for i in range(len(train_x))]\n",
    "\n",
    "# Predict \n",
    "pred = neigh.predict([new_ncd])\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method, being 75% accurate only, is going to make a lot of mistakes. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
